import json
import openai
import os
from tqdm import tqdm

openai.api_key = ""  # Set your OpenAI API key here

created_questions = {}
new_cases = []

# === Type and conversion codes ===
TYPE_CODES = {
    "Text": "00",
    "Angular": "01",
    "CSV": "02",
    "Canvas": "03",
    "HTML": "04",
    "JSON": "05",
    "LaTeX": "06",
    "Markdown": "07",
    "Matplotlib": "08",
    "Mermaid": "09",
    "TOML": "10",
    "React": "11",
    "SVG": "12",
    "Tikz": "13",
    "Typst": "14",
    "Vega": "15",
    "Vue": "16",
    "XML": "17",
    "YAML": "18"
}

non_renderable_types = {"JSON", "XML", "CSV", "YAML", "TOML"}

# === Helper function to call OpenAI ===
client = openai.OpenAI(api_key=openai.api_key)

def call_gpt(prompt):
    response = client.chat.completions.create(
        model="o3-mini",
        messages=[{"role": "user", "content": prompt}],
        max_completion_tokens=2048,
        temperature=1
    )
    return response.choices[0].message.content.strip()

# === Step 1: Generate Structured Query ===
def generate_query(item):
    sample_query = item.get("query_example", "").strip()

    prompt = f"""
You are designing a new LLM query based on the following input/output types:

- Input type: {item['input_type']}
- Output type: {item['output_type']}

You must generate a new query that is structured like the example below, but with random and different content and setting.

Example Query:
{sample_query}

Now, generate a **new structured query** that using this format:

Please output {item['output_type']}:

Task:
<One-sentence summary of what the LLM should generate>

Feature Requirements:
- 5 to 10 very specific and measurable requirements(For example, describing What, Where, How)
- Each requirement must describe something clearly visible or structured in the output
- Do NOT use vague terms like "responsive", "clean", "well-styled", or "user-friendly"
- Do NOT mention or include any images, icons, or external assets
- Do NOT include code or wrap anything in triple backticks

Only return the structured query as plain text.
"""
    return call_gpt(prompt)

# === Step 2: Generate Raw Output Keywords ===
def generate_raw_output_metric(query, output_type):
    non_renderable_types = {"JSON", "XML", "CSV", "YAML", "TOML"}

    if output_type in non_renderable_types:
        # Add format-specific structure hint
        if output_type == "JSON":
            structure_hint = "The output will be a JSON object with possible nested keys and arrays."
        elif output_type == "YAML":
            structure_hint = "The output will follow YAML format with nested maps and lists."
        elif output_type == "TOML":
            structure_hint = "The output will follow TOML format using key-value pairs and nested tables."
        elif output_type == "XML":
            structure_hint = "The output will be XML using nested tags. Paths should reflect the tag hierarchy."
        elif output_type == "CSV":
            structure_hint = "The output will be a CSV table. Use 'rows[n][column]' to refer to values, or 'rows[*][column]' to refer to a whole column."

        prompt = f"""
Your task is to analyze the **query text below** and extract a list of key paths that must exist in some raw output generated by some other LLMs.
You are NOT generating output ‚Äî you are only listing keys to check for during evaluation.

The output type is {output_type}. {structure_hint}

Query:
{query}

Instructions:
- Use appropriate key path formatting based on {output_type}
- For nested structures, use bracketed notation (e.g., user[profile][email])
- For CSV, refer to rows and columns (e.g., rows[0][email])
- Only include keys clearly implied by the query ‚Äî do not invent
- Return a JSON array of strings
"""
    else:
        prompt = f"""
Your task is to extract exact keywords or values that must appear in the **raw output** generated by some LLM ‚Äî not the input query.

The output type is {output_type}, which is visually renderable (e.g., HTML, Canvas, LaTeX).

Query:
{query}

Guidelines:
- Do NOT extract phrases from the query itself.
- Instead, deduce what the LLM should generate based on the query itself ONLY and list specific strings or tokens that must be present in that output.
- These could include heading text, labels, static strings, or keywords visible in the raw code.
- Do NOT include instructions, formatting terms (like ‚Äúcentered title‚Äù), or general phrases, these will not appear in the raw output.
- ONLY include tokens that are expected to appear verbatim in the raw output based on the query ONLY.

Return as a JSON array of strings.
"""

    response = call_gpt(prompt)
    try:
        return json.loads(response)
    except json.JSONDecodeError:
        return []

def generate_vqa(query):
    prompt = f"""
You are given a structured LLM query:

{query}

Your task is to generate a set of 5‚Äì10 visual question-answer (VQA) pairs based on the **Feature Requirements** in the query.

- Do NOT Ask about code syntax, tags, commands, or formatting instructions (e.g., no mention of \\begin{{equation}}, \\frac{{}}, <div>, etc.)
- Do NOT Ask about interactive behavior (hover, click, tabs, animations)
- Do NOT Ask about content from other pages or external components

DO:
- Ask about things that would be visible in a screenshot of the rendered output
- Include questions about text content, layout, list counts, bold/italic styling, visible structure, labels, etc.
- Ensure every question is objective and easily answerable by visual inspection
- Extract the visual question/answer pairs from the Feature Requirements only, do not invent something that is not there. 

Return a JSON object with:
- Keys = visual questions
- Values = short expected answers

Example:
{{
  "What is the page title at the top?": "Travel Getaway",
  "How many bullet points are shown in the left column?": "4",
  "Is the first word in each step bolded?": "Yes"
}}
"""
    response = call_gpt(prompt)
    try:
        vqa = json.loads(response)
        if isinstance(vqa, dict):
            return {
                q.strip(): a.strip()
                for q, a in vqa.items()
                if isinstance(q, str) and isinstance(a, str) and q.strip() and a.strip()
            }
        return {}
    except json.JSONDecodeError:
        return {}

# === Main loop ===
with open("sample_cases.json", "r", encoding="utf-8") as f:
    sample_items = json.load(f)

for item in tqdm(sample_items, desc="Generating cases"):
    rendering_needed = item['output_type'] not in non_renderable_types
    existing_queries_str = "\n".join(created_questions.get(item['task_name'], []))

    input_code = TYPE_CODES.get(item['input_type'], "XX")
    output_code = TYPE_CODES.get(item['output_type'], "XX")

    for example_index in range(50): 
        try:
            task_id = f"{input_code}{output_code}{example_index:02d}"

            # Step 1: Generate Query
            query = generate_query(item)
            print(f"\n[Query] {query}")

            # Step 2: Raw Output Keywords
            raw_output_metric = generate_raw_output_metric(query, item['output_type'])
            print(f"[Raw Keywords] {raw_output_metric}")

            # Step 3: VQA
            vqa_data = generate_vqa(query) if rendering_needed else {}
            print(f"[VQA] {vqa_data}")

            new_case = {
                "task_name": item['task_name'],
                "query_example": item['query_example'],
                "task_id": task_id,
                "input_type": item['input_type'],
                "output_type": item['output_type'],
                "rendering": rendering_needed,
                "query": query,
                "raw_output_metric": raw_output_metric,
                "VQA": vqa_data
            }

            new_cases.append(new_case)

            if item['task_name'] not in created_questions:
                created_questions[item['task_name']] = []
            created_questions[item['task_name']].append(query)

            print(f"‚úÖ Generated case for: {item['task_name']}")
        except Exception as e:
            print(f"‚ùå Error generating case for {item['task_name']}: {str(e)}")

# Save result
with open("generated_cases.json", "w", encoding="utf-8") as f:
    json.dump(new_cases, f, indent=4, ensure_ascii=False)

print(f"\nüéâ Successfully generated {len(new_cases)} test cases.")